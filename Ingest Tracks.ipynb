{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207086b5-c3ae-436d-a780-ea9ebfb55a05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99916fa1-d74c-4e34-bed3-e5a5c5d99d31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'Authorization': 'Bearer BQCRoyWh_yrMV8JRA3YsbsqZw-Clu2sNohYnm_8P1YwZ-cIabmoQythxRyk1ISmM7zypBEOP5VQ268VcfhqlJdd87ODqbi9H2iTHXCAX6avaWe0U42o'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|             TrackId|\n+--------------------+\n|                NULL|\n|00QuXoF6LyKwH9xz7...|\n|05xJcmdgeGqVG4HyR...|\n|06o5vKyvBS3dCTZrR...|\n|07qSl2sNpFNILaYyZ...|\n|09Z6o1yu596HJAwWZ...|\n|0BJkSAC37xEAJW2a9...|\n|0BRHnOFm6sjxN1i9L...|\n|0dEKHXkMPrC5vQkjP...|\n|0DNlORBnWpHXS9OmG...|\n|0f7XBGx85NV845Ufc...|\n|0fp8XEUc9Yxxd1Ago...|\n|0gktq7d62pEiregKV...|\n|0HDfN1Q5AukdgzQEG...|\n|0Hn9iSg0dwEfw7a7o...|\n|0iJPMG79hgWnDHjuf...|\n|0IlARMBAts1JxyYxD...|\n|0IRHbnOZpYbR1gc1k...|\n|0IxxqsYBcCHEQ1HqL...|\n|0JL9TZip7mL7iwC5E...|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#token = \"Bearer \"\n",
    "\n",
    "token = dbutils.widgets.get('token')\n",
    "tracks_id_index = 0\n",
    "headers = {\"Authorization\": token}\n",
    "display(headers)\n",
    "\n",
    "# Assuming you have a CSV file uploaded to Databricks File System (DBFS)\n",
    "csv_file_path = \"/mnt/raw/track_data/ds_tracks_ids_raw.csv\"\n",
    "\n",
    "# Read CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Display the DataFrame to see its structure\n",
    "df.show()\n",
    "\n",
    "# Collect Tracks ID as a list\n",
    "tracks_ids = df.select(\"TrackId\").rdd.flatMap(lambda x: x).collect()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d359e3-3a4a-4868-bb00-73ab248c8822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#OLD WITH NEW EXEPTION HANDLING\n",
    "\n",
    "start_index = tracks_id_index\n",
    "current_index = start_index  # Initialize current_index\n",
    "\n",
    "# Define custom exception\n",
    "class CustomException(Exception):\n",
    "    def __init__(self, message, index, response_code, retry_after=None):\n",
    "        self.message = message\n",
    "        self.index = index\n",
    "        self.response_code = response_code\n",
    "        self.retry_after = retry_after\n",
    "\n",
    "# Function to handle custom exception and retry logic\n",
    "def handle_exception(exception, retry_limit=3):\n",
    "    print(exception.message)\n",
    "    if exception.response_code == 429 and exception.retry_after:\n",
    "        print(f\"Retrying after {exception.retry_after} seconds...\")\n",
    "        time.sleep(exception.retry_after)  # Wait for the suggested time\n",
    "        retry_limit -= 1\n",
    "        if retry_limit > 0:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "        \n",
    "# Iterate over tracks IDs\n",
    "for tracks_id in tracks_ids[start_index:]:\n",
    "    offset = 0\n",
    "    limit = 50\n",
    "    search_url = f\"https://api.spotify.com/v1/tracks/{tracks_id}\"\n",
    "    retries = 3  # Number of retries\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            result = requests.get(search_url, headers=headers)\n",
    "            response_code = result.status_code  # Capture the response code\n",
    "            print(response_code)\n",
    "            \n",
    "            if response_code == 200:\n",
    "                track = json.loads(result.content)\n",
    "                print(track)\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                #Save album tracks to a JSON file in /mnt/incoming-tracks\n",
    "                filename =f\"/dbfs/mnt/incoming-tracks/{tracks_id}_incoming-tracks_{timestamp}.json\"\n",
    "                with open(filename, 'w') as json_file:\n",
    "                    json.dump(track, json_file, indent=2)\n",
    "                current_index += 1\n",
    "                break\n",
    "        except CustomException as e:\n",
    "            if not handle_exception(e):\n",
    "                break  # Break the loop if not retrying\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break  # Break the loop on unexpected errors\n",
    "        retries -= 1\n",
    "\n",
    "\n",
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingest Tracks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
