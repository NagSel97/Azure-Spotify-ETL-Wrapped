{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b123d76b-43d4-44ae-912b-7ff226295ff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import substring\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75eb3a64-1f92-4f47-978d-ada3dd1fac34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exemple de lecture CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for user streaming history DataFrame\n",
    "user_streaming_history_schema = StructType([\n",
    "    StructField(\"StreamingId\", IntegerType(), True),\n",
    "    StructField(\"TrackId\", StringType(), True),\n",
    "    StructField(\"Timestamp\", StringType(), True),\n",
    "    StructField(\"Username\", StringType(), True),\n",
    "    StructField(\"Platform\", StringType(), True),\n",
    "    StructField(\"Timeplayed\", IntegerType(), True),\n",
    "    StructField(\"Conncountry\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the associative schema for user streaming history and track DataFrame\n",
    "streaming_track_schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"StreamingId\", StringType(), True),\n",
    "    StructField(\"TrackId\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame for each defined schema\n",
    "df_streaming = spark.createDataFrame([], schema=user_streaming_history_schema)\n",
    "df_streaming_track = spark.createDataFrame([], schema=streaming_track_schema)\n",
    "\n",
    "# Read CSV file\n",
    "file_path = '/mnt/raw/user_data/test_user_data.csv'\n",
    "df_csv = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Get current timestamp\n",
    "start_time = time.time()\n",
    "\n",
    "# The streaming_id\n",
    "streaming_count = 0\n",
    "\n",
    "track_data = set()\n",
    "\n",
    "# Need to create the associative table\n",
    "streaming_track_dicts = {}\n",
    "\n",
    "# Iterate over each row in the CSV DataFrame\n",
    "for row in df_csv.rdd.collect():\n",
    "    # Increment the streaming Id\n",
    "    streaming_count += 1\n",
    "    streamingId = int(streaming_count)\n",
    "    trackId = str(row['spotify_track_uri'])\n",
    "    timestamp = str(row['ts'])\n",
    "    username = str(row['username'])\n",
    "    platform = str(row['platform'])\n",
    "    timeplayed = int(row['ms_played'])\n",
    "    conncountry = str(row['conn_country'])\n",
    "\n",
    "    \n",
    "    # Create a Spark DataFrame Row\n",
    "    streaming_row = Row(StreamingId=streamingId, TrackId=trackId, Timestamp=timestamp, \n",
    "                        Username=username, Platform=platform, Timeplayed=timeplayed, \n",
    "                        Conncountry=conncountry)\n",
    "    \n",
    "    track_data.add(trackId)\n",
    "    streaming_track_dicts[streaming_count] = trackId\n",
    "\n",
    "    # Append the Row to the user streaming DataFrame\n",
    "    df_streaming = df_streaming.union(spark.createDataFrame([streaming_row], schema=user_streaming_history_schema))\n",
    "\n",
    "#print(streaming_track_dicts)\n",
    "\n",
    "\n",
    "streaming_track_id = 0\n",
    "\n",
    "# Iterate over each track from the set to create the associative dataFrame\n",
    "for trackId in track_data:\n",
    "\n",
    "    # For each track I iterate over the streaming_track_dicts dictonnary \n",
    "    for stream in streaming_track_dicts:\n",
    "        if (trackId == streaming_track_dicts[stream]):\n",
    "            # Increment the streaming_track Id\n",
    "            streaming_track_id += 1\n",
    "\n",
    "            # Create a Spark DataFrame Row\n",
    "            streaming_track_row = Row(ID=streaming_track_id,StreamingId=stream,TrackId=trackId)\n",
    "\n",
    "            # Append the Row to the user streaming history and track DataFrame\n",
    "            df_streaming_track = df_streaming_track.union(spark.createDataFrame([streaming_track_row], schema=streaming_track_schema))\n",
    "\n",
    "# Print elapsed time\n",
    "print(\"Elapsed time:\", time.time() - start_time)\n",
    "\n",
    "# df\n",
    "df_streaming = df_streaming.withColumn('TrackId', substring('TrackId',15,23))\n",
    "df_streaming_track = df_streaming_track.withColumn('TrackId', substring('TrackId',15,23))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41049028-8090-45d1-b8d0-5827c765a459",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show DataFrame schema and data\n",
    "df_streaming.printSchema()\n",
    "df_streaming.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1220f58c-5b2e-470e-b8ae-bba13e4e8064",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show DataFrame schema and data\n",
    "df_streaming_track.printSchema()\n",
    "df_streaming_track.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44798071-04a7-4d4e-aed8-d654a2731218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write the transformed file ADL Processed Folder\n",
    "df_streaming.write.mode(\"overwrite\").csv(\"/mnt/processed/processed-user-data\")\n",
    "df_streaming_track.write.mode(\"overwrite\").csv(\"/mnt/processed/processed-user-track\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform User Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
